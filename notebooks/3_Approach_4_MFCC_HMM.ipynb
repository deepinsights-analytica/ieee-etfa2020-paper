{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 5 of 5\n",
    "# Machine Learning Based Unbalance Detection of a Rotating Shaft Using Vibration Data\n",
    "### Oliver Mey, Willi Neudeck, AndrÃ© Schneider and Olaf Enge-Rosenblatt\n",
    "##### Fraunhofer IIS/EAS, Fraunhofer Institute for Integrated Circuits, Division Engineering of Adaptive Systems, Dresden, Germany\n",
    "\n",
    "This Jupyter Notebook is part of a paper submission to the 25th IEEE International Conference on Emerging Technologies and Factory Automation, ETFA 2020. The notebook is the fifth one in a series of five freely available notebooks. It contains Python code fragments which were used to get the classification results described within the ETFA paper.\n",
    "\n",
    "*Last Update: August 2020*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 4: Hidden Markov Model, with MFCC features\n",
    "\n",
    "This notebook requires several dependencies, which can be installed in a conda environment using the following `environment.yml`:\n",
    "\n",
    "~~~~~ yaml\n",
    "name: hmm\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - h5py\n",
    "  - hmmlearn=0.2.3\n",
    "  - jupyter\n",
    "  - librosa=0.7.2\n",
    "  - matplotlib\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - python=3.8.*\n",
    "  - scikit-learn=0.22.*\n",
    "  - scipy\n",
    "  - dask\n",
    "  - dask-jobqueue\n",
    "  - conda\n",
    "  - numba=0.48.0\n",
    "~~~~~\n",
    " \n",
    "In addition, it is assumed that the notebook is run on a SLURM cluster (for fast hyperparameter search). It should be easy to modify the notebook to work without a cluster (see the comments in the concerned code cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import types\n",
    "import librosa.feature\n",
    "import librosa.filters\n",
    "import hmmlearn.hmm\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "import functools\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import dask_jobqueue\n",
    "import dask.distributed\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire dataset (about 2.7 GB) is freely available via the Fraunhofer Fortadis data space (https://fordatis.fraunhofer.de/handle/fordatis/151.2). To reproduce the HMM/MFCC results from the paper, download the ZIP-file from Fordatis and set the `infile`-variable below to the path of the ZIP-file. The datasets includes recordings for 4 different unbalance strengths (1D/1E ... 4D/4E) as well as one recording without unbalance (0D/0E). The recordings that will be used for training can be selected using the `n_good` and `n_bad` variables.\n",
    "\n",
    "The HMM/MFCC approach is believed to be sensitive for variations in the rotation speed. Therefore, a suitable speed range has to be selected using `rpm_lb` and `rpm_ub`. The recordings for training/development (0D ... 4D) cover the speed range approximately 630 ... 2340 rpm in steps of about 10 rpm. The recordings for evaluation (0E ... 4E) cover approximately 1060 ... 1930 rpm in steps of about 20 rpm.\n",
    "\n",
    "To reproduce the data in the paper, the notebook has to be run multiple times, with different values for `rpm_lb` and `rpm_ub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_good, n_bad = '0D', '3D'  # Recordings used for training.\n",
    "rpm_lb = 715  # Train model for this range of rotation speed: Lower bound, in RPM.\n",
    "rpm_ub = 815  # Upper bound, in RPM.\n",
    "win_len_ms = 1000  # Length of one sample, in milliseconds.\n",
    "\n",
    "infile = 'fraunhofer_eas_dataset_for_unbalance_detection_v1.zip'  # Set this to the path of the data file!\n",
    "skip = 50000  # Skip this number of measurements at the start of each recording.\n",
    "f_sampl = 4096  # Sampling rate, in Hz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "The following cells load the data from the ZIP-file. Currently, only one vibration signal is used (\"Vibration_1\"). The data is reshaped for convenience (one \"sample\" of length `win_len_ms` per row), and the samples are permuted randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_zfile(zfile, n):\n",
    "    win_len = int(win_len_ms/1000 * f_sampl)\n",
    "    with zfile.open(n + '.csv', 'r') as f:\n",
    "        data = pd.read_csv(f).iloc[skip:, :]\n",
    "    n = (data.shape[0]//win_len) * win_len\n",
    "    data = data.iloc[:n, :]\n",
    "    rpm = np.reshape(data['Measured_RPM'].values, (-1, win_len), order='C')\n",
    "    vibr = np.reshape(data['Vibration_1'].values, (-1, win_len), order='C')\n",
    "    ind, = np.nonzero(np.all(rpm > rpm_lb, axis=1) & np.all(rpm < rpm_ub, axis=1))\n",
    "    np.random.seed(170287); ind = np.random.permutation(ind)\n",
    "    return vibr[ind,:].copy()\n",
    "\n",
    "def load_data(filename, n_good, n_bad):\n",
    "    with zipfile.ZipFile(filename, 'r') as zfile:\n",
    "        good = load_from_zfile(zfile, n_good)\n",
    "        bad = load_from_zfile(zfile, n_bad)\n",
    "    return good, bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good, bad = load_data(infile, n_good, n_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of \"good\" samples', good.shape[0])\n",
    "print('Number of \"bad\" samples', bad.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routines for Training + Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    f_sampl,  # Sampling rate, in Hz.\n",
    "    good,  # \"Good\" training data (i.e. without aunbalance).\n",
    "    bad,  # \"Bad\" training data (i.e. with unbalance).\n",
    "    *,\n",
    "    \n",
    "    r_train = 0.5,  # Ratio of the \"good\" training data, that will be used\n",
    "                    # to train the Hidden markov Model (the Logistic Regression\n",
    "                    # will be trained using separate data).\n",
    "\n",
    "    # The following hyperparameters will be optimized, later.\n",
    "    fft_win_ms = 31.25,  # Length of one FFT window, in milliseconds.\n",
    "    hop_len_ms = 8.0,  # Displacement of consecutive FFT windows, in milliseconds.\n",
    "    n_mels = 15,  # Number of mel filters.\n",
    "    hmm_states = 5,  # Number of states in the Hidden Markov Model.\n",
    "):\n",
    "    n_good = int(good.shape[0] * r_train)\n",
    "    n_bad = int(bad.shape[0] * r_train)\n",
    "\n",
    "    nfft = int(fft_win_ms/1000 * f_sampl)\n",
    "    hop_len = int(hop_len_ms/1000 * f_sampl)\n",
    "    mfcc_args = dict(sr=f_sampl, n_fft=nfft, n_mels=n_mels, hop_length=hop_len)\n",
    "\n",
    "    # Step I:\n",
    "    # Compute (scaled) features + train HMM for a portion of the \"good\" training data.\n",
    "    \n",
    "    tmp = [librosa.feature.mfcc(good[i, :], **mfcc_args).T\n",
    "           for i in range(n_good)]\n",
    "    feat_train = np.concatenate(tmp, axis=0)\n",
    "    len_train = [m.shape[0] for m in tmp]\n",
    "\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler.fit(feat_train)\n",
    "    model = hmmlearn.hmm.GaussianHMM(n_components=hmm_states)\n",
    "    model.fit(scaler.transform(feat_train), lengths=len_train)\n",
    "    \n",
    "    # Step II:\n",
    "    # Compute HMM output for a second portion of the \"good\" + \"bad\" training data,\n",
    "    # then train a LogisticRegression on the (scaled) HMM output.\n",
    "\n",
    "    tmp1 = [model.score(scaler.transform(\n",
    "                librosa.feature.mfcc(good[i, :], **mfcc_args).T))\n",
    "            for i in range(n_good, good.shape[0])]\n",
    "    tmp2 = [model.score(scaler.transform(\n",
    "                librosa.feature.mfcc(bad[i, :], **mfcc_args).T))\n",
    "            for i in range(n_bad, bad.shape[0])]\n",
    "\n",
    "    lr = sklearn.pipeline.make_pipeline(\n",
    "        sklearn.preprocessing.StandardScaler(),\n",
    "        sklearn.linear_model.LogisticRegression(),\n",
    "    )\n",
    "    lr.fit(\n",
    "        np.concatenate([np.reshape(tmp1, (-1,1)), np.reshape(tmp2, (-1,1))], axis=0),\n",
    "        np.array([0]*len(tmp1) + [1]*len(tmp2))\n",
    "    )\n",
    "    \n",
    "    return mfcc_args, model, scaler, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, good, bad):\n",
    "    mfcc_args, model, scaler, lr = models\n",
    "    \n",
    "    tmp1 = [model.score(scaler.transform(\n",
    "                librosa.feature.mfcc(good[i, :], **mfcc_args).T))\n",
    "            for i in range(good.shape[0])]\n",
    "    tmp2 = [model.score(scaler.transform(\n",
    "                librosa.feature.mfcc(bad[i, :], **mfcc_args).T))\n",
    "            for i in range(bad.shape[0])]\n",
    "\n",
    "    result = lr.predict(\n",
    "        np.concatenate([np.reshape(tmp1, (-1,1)), np.reshape(tmp2, (-1,1))], axis=0))\n",
    "    actual = np.array([0]*len(tmp1) + [1]*len(tmp2))\n",
    "\n",
    "    return sklearn.metrics.balanced_accuracy_score(actual, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following routine performs a train/test split before performing training + test. It returns the test result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(\n",
    "    f_sampl, good, bad, *,\n",
    "    n_train=200,  # Number of samples for training.\n",
    "    **kwargs\n",
    "):\n",
    "    m = train(f_sampl, good[:n_train,:], bad[:n_train,:], **kwargs)\n",
    "    return test(m, good[n_train:,:], bad[n_train:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter-Search on cluster\n",
    "\n",
    "We use a SLURM cluster for hyperparameter search. To run this notebook without a cluster, some cells in this section have to be modified according to the comments in the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option (a): Use SLURM cluster.\n",
    "slurm = dask_jobqueue.SLURMCluster(cores=1, memory='2GB')\n",
    "slurm.scale(100)\n",
    "client = dask.distributed.Client(slurm)\n",
    "display(client)\n",
    "# Option (b): Without cluster.\n",
    "#client = dask.distributed.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute `get_score()` for the cartesian product of the following hyperparameter settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_states_ = [1,2,3,4,5]\n",
    "n_mels_ = [10,15,20] #[1,2,5,10,15]\n",
    "fft_win_ms_ = [10,30,100,300]\n",
    "hop_len_ms_ = [5,10,20,50]\n",
    "\n",
    "args = list(itertools.product(hmm_states_, n_mels_, fft_win_ms_, hop_len_ms_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def f(a, good, bad):\n",
    "    hmm_states, n_mels, fft_win_ms, hop_len_ms = a\n",
    "    return get_score(\n",
    "        f_sampl, good, bad,\n",
    "        hmm_states=hmm_states,\n",
    "        n_mels=n_mels,\n",
    "        fft_win_ms=fft_win_ms,\n",
    "        hop_len_ms=hop_len_ms,\n",
    "    )\n",
    "\n",
    "tmp1 = client.scatter(good)\n",
    "tmp2 = client.scatter(bad)\n",
    "scores = client.gather(client.map(f, args, good=tmp1, bad=tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option (a): Use SLURM cluster.\n",
    "slurm.scale(0)  # Stop jobs on the cluster.\n",
    "# Option (b): Without cluster.\n",
    "# (Just remove the `slurm.scale(0)` line above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the result of the hyperparameter search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_states, n_mels, fft_win_ms, hop_len_ms = args[np.argmax(scores)]\n",
    "print('Best hyperparameters:', hmm_states, n_mels, fft_win_ms, hop_len_ms)\n",
    "\n",
    "get_score(\n",
    "    f_sampl, good, bad,\n",
    "    hmm_states=hmm_states,\n",
    "    n_mels=n_mels,\n",
    "    fft_win_ms=fft_win_ms,\n",
    "    hop_len_ms=hop_len_ms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-train best model on all training data + evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(\n",
    "    f_sampl, good, bad,\n",
    "    hmm_states=hmm_states,\n",
    "    n_mels=n_mels,\n",
    "    fft_win_ms=fft_win_ms,\n",
    "    hop_len_ms=hop_len_ms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_good = '0D'\n",
    "for n_bad in ('1D', '2D', '3D', '4D'):\n",
    "    good, bad = load_data(infile, n_good, n_bad)\n",
    "    score = test(model, good, bad)\n",
    "    print(f'{rpm_lb} < rpm < {rpm_ub}; '\n",
    "          f'\"{n_good}\" (n = {good.shape[0]}) vs. \"{n_bad}\" (n = {bad.shape[0]}):'\n",
    "          f' balanced accuracy = {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_good = '0E'\n",
    "for n_bad in ('1E', '2E', '3E', '4E'):\n",
    "    good, bad = load_data(infile, n_good, n_bad)\n",
    "    score = test(model, good, bad)\n",
    "    print(f'{rpm_lb} < rpm < {rpm_ub}; '\n",
    "          f'\"{n_good}\" (n = {good.shape[0]}) vs. \"{n_bad}\" (n = {bad.shape[0]}):'\n",
    "          f' balanced accuracy = {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model[1].transmat_)\n",
    "plt.clim(0,1)\n",
    "plt.xticks(np.arange(hmm_states))\n",
    "plt.yticks(np.arange(hmm_states))\n",
    "plt.colorbar()\n",
    "plt.ylabel('Old HMM state')\n",
    "plt.xlabel('New HMM state')\n",
    "plt.title('State-transistion probabilities of HMM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model[1].means_)\n",
    "plt.xlabel('MFCCs (scaled + centered)')\n",
    "plt.ylabel('HMM state')\n",
    "plt.yticks(np.arange(hmm_states))\n",
    "plt.title('Values of the MFCCs in the HMM-states')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
